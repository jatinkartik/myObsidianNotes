# Master Node and Driver Program in Apache Spark

- **Master Node:**
  - The driver program drives our application.
  - The code we write acts as the driver program.

- **Driver Program:**
  - First, create Spark contexts inside the driver program.
  - Spark contexts act as gateways to all Spark functionalities.

- **Spark Contexts:**
  - Similar to running a command in a database, anything we do in Apache Spark goes through Spark contexts.
  - Spark contexts function as cluster managers, with the driver program handling job processing within the cluster.
  - Jobs are distributed to worker nodes in the cluster, and the results are returned to Spark contexts.

# Features of Apache Spark:

1. **Speed:**
   - Spark runs up to 100 times faster than Hadoop for large-scale data processing.

2. **Real-time:**
   - Offers real-time computation and low latency due to in-memory computation.

3. **Fault Tolerance:**
   - Can recover lost data by recomputing using Resilient Distributed Datasets (RDD).

4. **Ecosystem:**
   - Rich ecosystem with libraries and tools, including Spark SQL, machine learning libraries, GraphX for graph processing, and Spark Streaming for real-time data processing.

5. **Lazy Evaluation:**
   - Spark uses lazy evaluation, delaying the execution of operations for optimization purposes.
